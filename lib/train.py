# ============================================
# 8. å¿«é€Ÿè®­ç»ƒè„šæœ¬
# ============================================

# import sys
# import os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# import torch
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from lib.dataset import TrackingDataset

# def quick_train():
#     """ç®€åŒ–çš„è®­ç»ƒæµç¨‹"""
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     model = RGBDTextTracker().to(device)

#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

#     # 1. æŒ‡å®šæ­£ç¡®è·¯å¾„ & split
#     train_loader = DataLoader(
#         TrackingDataset(data_root='/data/depth/aic25', split='train'),
#         batch_size=16,
#         shuffle=True,
#         num_workers=4,
#         pin_memory=True
#     )

#     os.makedirs('outputs/exp1/ckpt', exist_ok=True)

#     for epoch in range(50):
#         model.train()
#         epoch_loss = 0.
#         for batch in train_loader:
#             # 2. ç»Ÿä¸€ to(device)
#             tpl_rgb   = batch['template_rgb'].to(device)
#             tpl_dep   = batch['template_depth'].to(device)
#             srh_rgb   = batch['search_rgb'].to(device)
#             srh_dep   = batch['search_depth'].to(device)
#             bbox_gt   = batch['bbox'].to(device)

#             pred_bbox, _ = model(tpl_rgb, tpl_dep, batch['text'], srh_rgb, srh_dep)

#             loss = F.l1_loss(pred_bbox, bbox_gt)

#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()
#             epoch_loss += loss.item()

#         avg_loss = epoch_loss / len(train_loader)
#         print(f"Epoch {epoch:02d}  Avg-Loss: {avg_loss:.6f}")

#         # 3. æ¯ 10 è½®ä¿å­˜æƒé‡ï¼ˆä¾›åé¢æ¨ç†ç”¨ï¼‰
#         if epoch % 5 == 0 or epoch == 49:
#             ckpt_path = f'outputs/exp1/ckpt2/epoch{epoch}.pth'
#             torch.save(model.state_dict(), ckpt_path)
#             print(f"  â†’  saved {ckpt_path}")

#     # è®­ç»ƒå®ŒæŠŠã€Œæœ€ä½³ã€é“¾æ¥åˆ° best.pthï¼Œæ–¹ä¾¿æ¨ç†è„šæœ¬ç›´æ¥åŠ è½½
#     torch.save(model.state_dict(), 'best.pth')
#     print('All done! best.pth ready for inference.')

# if __name__ == "__main__":
#     quick_train()
# lib/train.py
# import sys, os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
# import torch
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from lib.dataset import TrackingDataset

# def giou_loss(pred_bbox, gt_bbox):
#     """GIoUæŸå¤±(æ›´é€‚åˆè·Ÿè¸ª)"""
#     # è½¬æ¢ä¸ºxyxyæ ¼å¼
#     pred_x1, pred_y1 = pred_bbox[:, 0], pred_bbox[:, 1]
#     pred_x2, pred_y2 = pred_x1 + pred_bbox[:, 2], pred_y1 + pred_bbox[:, 3]
    
#     gt_x1, gt_y1 = gt_bbox[:, 0], gt_bbox[:, 1]
#     gt_x2, gt_y2 = gt_x1 + gt_bbox[:, 2], gt_y1 + gt_bbox[:, 3]
    
#     # äº¤é›†
#     inter_x1 = torch.max(pred_x1, gt_x1)
#     inter_y1 = torch.max(pred_y1, gt_y1)
#     inter_x2 = torch.min(pred_x2, gt_x2)
#     inter_y2 = torch.min(pred_y2, gt_y2)
    
#     inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    
#     # å¹¶é›†
#     pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
#     gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
#     union_area = pred_area + gt_area - inter_area
    
#     iou = inter_area / (union_area + 1e-7)
    
#     # æœ€å°å¤–æ¥çŸ©å½¢
#     enclose_x1 = torch.min(pred_x1, gt_x1)
#     enclose_y1 = torch.min(pred_y1, gt_y1)
#     enclose_x2 = torch.max(pred_x2, gt_x2)
#     enclose_y2 = torch.max(pred_y2, gt_y2)
    
#     enclose_area = (enclose_x2 - enclose_x1) * (enclose_y2 - enclose_y1)
    
#     # GIoU
#     giou = iou - (enclose_area - union_area) / (enclose_area + 1e-7)
    
#     return 1 - giou.mean()

# def quick_train():
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#     # # âœ… æ‰“å°åˆå§‹æ˜¾å­˜
#     # print(f"[INFO] åˆå§‹æ˜¾å­˜: {torch.cuda.memory_allocated()/1e9:.2f}GB / {torch.cuda.get_device_properties(0).total_memory/1e9:.2f}GB")
    
#     model = RGBDTextTracker().to(device)

#     #  # âœ… æ‰“å°æ¨¡å‹æ˜¾å­˜
#     # print(f"[INFO] æ¨¡å‹åŠ è½½åæ˜¾å­˜: {torch.cuda.memory_allocated()/1e9:.2f}GB")

#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
#     train_loader = DataLoader(
#         TrackingDataset(data_root='/data/depth/aic25', split='train', k=40),
#         batch_size=16,  # å‡å°batché€‚åº”Transformer
#         shuffle=True,
#         num_workers=8,
#         pin_memory=True
#     )
    
#     os.makedirs('outputs/exp_jvg/ckpt', exist_ok=True)
#     best_loss = float('inf')
    
#     for epoch in range(50):
#         model.train()
#         epoch_loss = 0.
        
#         for batch_idx, batch in enumerate(train_loader):
#             tpl_rgb = batch['template_rgb'].to(device)
#             tpl_dep = batch['template_depth'].to(device)
#             srh_rgb = batch['search_rgb'].to(device)
#             srh_dep = batch['search_depth'].to(device)
#             bbox_gt = batch['bbox'].to(device)
            
#             pred_bbox, _ = model(tpl_rgb, tpl_dep, batch['text'], srh_rgb, srh_dep)
            
#             # === ç»„åˆæŸå¤± ===
#             l1_loss = F.l1_loss(pred_bbox, bbox_gt)
#             giou_loss_val = giou_loss(pred_bbox, bbox_gt)
#             loss = l1_loss + 3.0 * giou_loss_val  # GIoUæƒé‡æ›´å¤§
            
#             optimizer.zero_grad()
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
#             optimizer.step()
            
#             epoch_loss += loss.item()
            
#             # if batch_idx % 50 == 0:
#             #     print(f"  Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f}")
        
#         scheduler.step()
#         avg_loss = epoch_loss / len(train_loader)
#         print(f"Epoch {epoch:02d}  Loss: {avg_loss:.4f}  LR: {scheduler.get_last_lr()[0]:.6f}")
        
#         if avg_loss < best_loss:
#             best_loss = avg_loss
#             torch.save(model.state_dict(), 'best.pth')
#             print(f"  â†’ Best model saved (loss: {best_loss:.4f})")
        
#         if epoch % 5 == 0 or epoch == 49:
#             torch.save(model.state_dict(), f'outputs/exp_jvg/ckpt/epoch{epoch}.pth')
    
#     print('Training done!')

# if __name__ == "__main__":
#     quick_train()

# lib/train.py
# import sys, os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
# import torch
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from lib.dataset import TrackingDataset

# def giou_loss(pred_bbox, gt_bbox):
#     """GIoUæŸå¤±"""
#     # âœ… ç¡®ä¿è¾“å…¥æ ¼å¼ï¼š[x,y,w,h]
#     pred_x1 = pred_bbox[:, 0]
#     pred_y1 = pred_bbox[:, 1]
#     pred_x2 = pred_bbox[:, 0] + pred_bbox[:, 2]
#     pred_y2 = pred_bbox[:, 1] + pred_bbox[:, 3]
    
#     gt_x1 = gt_bbox[:, 0]
#     gt_y1 = gt_bbox[:, 1]
#     gt_x2 = gt_bbox[:, 0] + gt_bbox[:, 2]
#     gt_y2 = gt_bbox[:, 1] + gt_bbox[:, 3]
    
#     # äº¤é›†
#     inter_x1 = torch.max(pred_x1, gt_x1)
#     inter_y1 = torch.max(pred_y1, gt_y1)
#     inter_x2 = torch.min(pred_x2, gt_x2)
#     inter_y2 = torch.min(pred_y2, gt_y2)
    
#     inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    
#     pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
#     gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
#     union_area = pred_area + gt_area - inter_area + 1e-7
    
#     iou = inter_area / union_area
    
#     # æœ€å°å¤–æ¥çŸ©å½¢
#     enclose_x1 = torch.min(pred_x1, gt_x1)
#     enclose_y1 = torch.min(pred_y1, gt_y1)
#     enclose_x2 = torch.max(pred_x2, gt_x2)
#     enclose_y2 = torch.max(pred_y2, gt_y2)
    
#     enclose_area = (enclose_x2 - enclose_x1) * (enclose_y2 - enclose_y1) + 1e-7
    
#     giou = iou - (enclose_area - union_area) / enclose_area
    
#     return 1 - giou.mean()

# def quick_train():
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     model = RGBDTextTracker().to(device)
    
#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
#     # âœ… å…³é”®ï¼šç¡®ä¿datasetè¿”å›256x256å°ºåº¦çš„bbox
#     train_loader = DataLoader(
#         TrackingDataset(data_root='/data/depth/aic25', split='train', k=40),
#         batch_size=8,  # âœ… æé«˜åˆ°24
#         shuffle=True,
#         num_workers=8,
#         pin_memory=True
#     )
    
#     os.makedirs('outputs/exp_final/ckpt', exist_ok=True)
#     best_loss = float('inf')
    
#     for epoch in range(50):
#         model.train()
#         epoch_loss = 0.
        
#         for batch_idx, batch in enumerate(train_loader):
#             tpl_rgb = batch['template_rgb'].to(device)
#             tpl_dep = batch['template_depth'].to(device)
#             srh_rgb = batch['search_rgb'].to(device)
#             srh_dep = batch['search_depth'].to(device)
#             bbox_gt = batch['bbox'].to(device)
            
#             # âœ… å…³é”®ï¼šç¡®ä¿GT bboxä¹Ÿæ˜¯256x256å°ºåº¦
#             # å¦‚æœGTæ˜¯åŸå›¾å°ºåº¦ï¼Œéœ€è¦ç¼©æ”¾
#             # å‡è®¾åŸå›¾æ˜¯1920x1080ï¼Œéœ€è¦ç¼©æ”¾åˆ°256x256
#             # bbox_gt_scaled = bbox_gt * (256.0 / åŸå›¾å°ºå¯¸)
            
#             pred_bbox, _ = model(tpl_rgb, tpl_dep, batch['text'], srh_rgb, srh_dep)
            
#             # === ç»„åˆæŸå¤± ===
#             l1_loss = F.l1_loss(pred_bbox, bbox_gt)
#             giou_loss_val = giou_loss(pred_bbox, bbox_gt)
            
#             # âœ… æ·»åŠ å°ºåº¦çº¦æŸ
#             w_penalty = torch.mean(torch.abs(pred_bbox[:, 2] - bbox_gt[:, 2]))
#             h_penalty = torch.mean(torch.abs(pred_bbox[:, 3] - bbox_gt[:, 3]))
            
#             loss = l1_loss + 2.0 * giou_loss_val + 0.5 * (w_penalty + h_penalty)
            
#             optimizer.zero_grad()
#             # loss.backward()
#             # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
#             # optimizer.step()
            
#             epoch_loss += loss.item()
            
#             if batch_idx % 100 == 0:
#                 print(f"  Batch {batch_idx} Loss: {loss.item():.4f} L1: {l1_loss.item():.2f} GIoU: {giou_loss_val.item():.2f}")
        
#         scheduler.step()
#         avg_loss = epoch_loss / len(train_loader)
#         print(f"Epoch {epoch:02d}  Loss: {avg_loss:.4f}  LR: {scheduler.get_last_lr()[0]:.6f}")
        
#         if avg_loss < best_loss:
#             best_loss = avg_loss
#             torch.save(model.state_dict(), 'best.pth')
#             print(f"  â†’ Best model saved (loss: {best_loss:.4f})")
        
#         if epoch % 5 == 0 or epoch == 49:
#             torch.save(model.state_dict(), f'outputs/exp_final/ckpt/epoch{epoch}.pth')
    
#     print('Training done!')

# if __name__ == "__main__":
#     quick_train()

# lib/train_final.py - ç®€åŒ–ç¨³å®šçš„è®­ç»ƒè„šæœ¬
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from models.tracker import RGBDTextTracker
from lib.dataset import TrackingDataset

def giou_loss(pred_bbox, gt_bbox):
    """GIoUæŸå¤± - 256å°ºåº¦"""
    pred_x1 = pred_bbox[:, 0]
    pred_y1 = pred_bbox[:, 1]
    pred_x2 = pred_bbox[:, 0] + pred_bbox[:, 2]
    pred_y2 = pred_bbox[:, 1] + pred_bbox[:, 3]
    
    gt_x1 = gt_bbox[:, 0]
    gt_y1 = gt_bbox[:, 1]
    gt_x2 = gt_bbox[:, 0] + gt_bbox[:, 2]
    gt_y2 = gt_bbox[:, 1] + gt_bbox[:, 3]
    
    # äº¤é›†
    inter_x1 = torch.max(pred_x1, gt_x1)
    inter_y1 = torch.max(pred_y1, gt_y1)
    inter_x2 = torch.min(pred_x2, gt_x2)
    inter_y2 = torch.min(pred_y2, gt_y2)
    
    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
    
    pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
    gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
    union_area = pred_area + gt_area - inter_area + 1e-7
    
    iou = inter_area / union_area
    
    # æœ€å°å¤–æ¥çŸ©å½¢
    enclose_x1 = torch.min(pred_x1, gt_x1)
    enclose_y1 = torch.min(pred_y1, gt_y1)
    enclose_x2 = torch.max(pred_x2, gt_x2)
    enclose_y2 = torch.max(pred_y2, gt_y2)
    
    enclose_area = (enclose_x2 - enclose_x1) * (enclose_y2 - enclose_y1) + 1e-7
    
    giou = iou - (enclose_area - union_area) / enclose_area
    
    return 1 - giou.mean()

def quick_train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = RGBDTextTracker().to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)
    
    # Datasetï¼ˆç°åœ¨GTå·²ç»æ˜¯256å°ºåº¦ï¼‰
    train_loader = DataLoader(
        TrackingDataset(data_root='/data/depth/aic25', split='train', k=30),
        batch_size=16,
        shuffle=True,
        num_workers=8,
        pin_memory=True
    )
    
    os.makedirs('outputs/exp_final/ckpt', exist_ok=True)
    best_loss = float('inf')
    
    for epoch in range(50):
        model.train()
        epoch_loss = 0.
        epoch_giou = 0.
        epoch_l1 = 0.
        
        for batch_idx, batch in enumerate(train_loader):
            tpl_rgb = batch['template_rgb'].to(device)
            tpl_dep = batch['template_depth'].to(device)
            srh_rgb = batch['search_rgb'].to(device)
            srh_dep = batch['search_depth'].to(device)
            bbox_gt = batch['bbox'].to(device)  # å·²ç»æ˜¯256å°ºåº¦
            
            # å‰å‘ä¼ æ’­
            pred_bbox, _ = model(tpl_rgb, tpl_dep, batch['text'], srh_rgb, srh_dep)
            
            # ===== å¤šä»»åŠ¡æŸå¤± =====
            # 1. GIoUæŸå¤±ï¼ˆä¸»è¦ï¼‰
            giou_loss_val = giou_loss(pred_bbox, bbox_gt)
            
            # 2. L1æŸå¤±ï¼ˆè¾…åŠ©ï¼Œå¸®åŠ©å¿«é€Ÿæ”¶æ•›ï¼‰
            l1_loss = F.smooth_l1_loss(pred_bbox, bbox_gt)
            
            # 3. ä¸­å¿ƒç‚¹æŸå¤±ï¼ˆæå‡å®šä½ç²¾åº¦ï¼‰
            pred_center = pred_bbox[:, :2] + pred_bbox[:, 2:] / 2
            gt_center = bbox_gt[:, :2] + bbox_gt[:, 2:] / 2
            center_loss = F.mse_loss(pred_center, gt_center)
            
            # ç»„åˆæŸå¤±
            loss = 2.0 * giou_loss_val + 1.0 * l1_loss + 0.5 * center_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            epoch_giou += giou_loss_val.item()
            epoch_l1 += l1_loss.item()
            
            if batch_idx % 1000 == 0:
                print(f"  Batch {batch_idx}/{len(train_loader)} "
                      f"Loss: {loss.item():.4f} "
                      f"GIoU: {giou_loss_val.item():.4f} "
                      f"L1: {l1_loss.item():.4f}")
        
        scheduler.step()
        avg_loss = epoch_loss / len(train_loader)
        avg_giou = epoch_giou / len(train_loader)
        avg_l1 = epoch_l1 / len(train_loader)
        
        # print(f"\n{'='*60}")
        # print(f"Epoch {epoch:02d}  "
        #       f"Loss: {avg_loss:.4f}  "
        #       f"GIoU: {avg_giou:.4f}  "
        #       f"L1: {avg_l1:.4f}  "
        #       f"LR: {scheduler.get_last_lr()[0]:.6f}")
        # print(f"{'='*60}\n")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(model.state_dict(), 'best.pth')
            print(f"  âœ… Best model saved (loss: {best_loss:.4f})")
        
        # å®šæœŸcheckpoint
        if epoch % 5 == 0 or epoch == 49:
            torch.save(model.state_dict(), f'outputs/exp_final/ckpt/epoch{epoch}.pth')
    
    print('\nğŸ‰ Training done!')

if __name__ == "__main__":
    quick_train()

# â†‘èƒ½è·‘é€šä½†å•å¡
# lib/train_final.py
# import sys, os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
# import torch
# import torch.nn.functional as F
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from lib.dataset import TrackingDataset


# def giou_loss(pred_bbox, gt_bbox):
#     pred_x1 = pred_bbox[:, 0]
#     pred_y1 = pred_bbox[:, 1]
#     pred_x2 = pred_bbox[:, 0] + pred_bbox[:, 2]
#     pred_y2 = pred_bbox[:, 1] + pred_bbox[:, 3]
#     gt_x1 = gt_bbox[:, 0]
#     gt_y1 = gt_bbox[:, 1]
#     gt_x2 = gt_bbox[:, 0] + gt_bbox[:, 2]
#     gt_y2 = gt_bbox[:, 1] + gt_bbox[:, 3]
#     inter_x1 = torch.max(pred_x1, gt_x1)
#     inter_y1 = torch.max(pred_y1, gt_y1)
#     inter_x2 = torch.min(pred_x2, gt_x2)
#     inter_y2 = torch.min(pred_y2, gt_y2)
#     inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)
#     pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)
#     gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)
#     union_area = pred_area + gt_area - inter_area + 1e-7
#     iou = inter_area / union_area
#     enclose_x1 = torch.min(pred_x1, gt_x1)
#     enclose_y1 = torch.min(pred_y1, gt_y1)
#     enclose_x2 = torch.max(pred_x2, gt_x2)
#     enclose_y2 = torch.max(pred_y2, gt_y2)
#     enclose_area = (enclose_x2 - enclose_x1) * (enclose_y2 - enclose_y1) + 1e-7
#     giou = iou - (enclose_area - union_area) / enclose_area
#     return 1 - giou.mean()


# def quick_train():
#     # âœ… å¤šå¡å®‰å…¨åˆå§‹åŒ–ï¼ˆç¨³å®š cudnnï¼‰
#     torch.backends.cudnn.enabled = True
#     torch.backends.cudnn.benchmark = False
#     torch.backends.cudnn.deterministic = True

#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#     # âœ… æ¨¡å‹åŠ è½½ä¸å¤šå¡å°è£…
#     model = RGBDTextTracker().to(device)
#     if torch.cuda.device_count() > 1:
#         print(f"ğŸ”§ Using {torch.cuda.device_count()} GPUs for training")
#         model = torch.nn.DataParallel(model)  # âœ… ä»…æ·»åŠ è¿™ä¸€è¡Œ
#     else:
#         print("âš™ï¸ Using single GPU")

#     # ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
#     optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)

#     # æ•°æ®é›†åŠ è½½
#     train_loader = DataLoader(
#         TrackingDataset(data_root='/data/depth/aic25', split='train', k=30),
#         batch_size=32,              # âœ… å»ºè®®æ”¹æˆèƒ½è¢«GPUæ•°æ•´é™¤çš„batch size
#         shuffle=True,
#         num_workers=8,
#         pin_memory=True
#     )

#     os.makedirs('outputs/exp_final/ckpt', exist_ok=True)
#     best_loss = float('inf')

#     for epoch in range(50):
#         model.train()
#         epoch_loss = epoch_giou = epoch_l1 = 0.0

#         for batch_idx, batch in enumerate(train_loader):
#             tpl_rgb = batch['template_rgb'].to(device)
#             tpl_dep = batch['template_depth'].to(device)
#             srh_rgb = batch['search_rgb'].to(device)
#             srh_dep = batch['search_depth'].to(device)
#             bbox_gt = batch['bbox'].to(device)

#             # å‰å‘ä¼ æ’­
#             pred_bbox, _ = model(tpl_rgb, tpl_dep, batch['text'], srh_rgb, srh_dep)

#             # å¤šä»»åŠ¡æŸå¤±
#             giou_loss_val = giou_loss(pred_bbox, bbox_gt)
#             l1_loss = F.smooth_l1_loss(pred_bbox, bbox_gt)
#             pred_center = pred_bbox[:, :2] + pred_bbox[:, 2:] / 2
#             gt_center = bbox_gt[:, :2] + bbox_gt[:, 2:] / 2
#             center_loss = F.mse_loss(pred_center, gt_center)
#             loss = 2.0 * giou_loss_val + 1.0 * l1_loss + 0.5 * center_loss

#             optimizer.zero_grad()
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
#             optimizer.step()

#             epoch_loss += loss.item()
#             epoch_giou += giou_loss_val.item()
#             epoch_l1 += l1_loss.item()

#             if batch_idx % 1000 == 0:
#                 print(f"  Batch {batch_idx}/{len(train_loader)} "
#                       f"Loss: {loss.item():.4f} "
#                       f"GIoU: {giou_loss_val.item():.4f} "
#                       f"L1: {l1_loss.item():.4f}")

#         scheduler.step()
#         avg_loss = epoch_loss / len(train_loader)
#         avg_giou = epoch_giou / len(train_loader)
#         avg_l1 = epoch_l1 / len(train_loader)

#         # ä¿å­˜æœ€ä½³æ¨¡å‹
#         if avg_loss < best_loss:
#             best_loss = avg_loss
#             torch.save(model.state_dict(), 'best.pth')
#             print(f"  âœ… Best model saved (loss: {best_loss:.4f})")

#         if epoch % 5 == 0 or epoch == 49:
#             torch.save(model.state_dict(), f'outputs/exp_final/ckpt/epoch{epoch}.pth')

#     print('\nğŸ‰ Training done!')


# if __name__ == "__main__":
#     quick_train()
