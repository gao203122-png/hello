# # ============================================
# # 6. test.py - æµ‹è¯•æ¨ç†
# # ============================================
# import os
# import glob
# import torch
# import cv2
# import numpy as np
# from models.tracker import RGBDTextTracker
# from models.text_encoder import TextEncoder
# from lib.dataset import preprocess_image, preprocess_depth, crop_template, crop_search_region, transform_bbox

# def test_sequence(model, sequence_path):
#     """æµ‹è¯•å•ä¸ªåºåˆ—"""
#     # è¯»å–ç¬¬ä¸€å¸§å’Œæ–‡æœ¬
#     rgb_list = sorted(glob.glob(f"{sequence_path}/rgb/*.jpg"))
#     depth_list = sorted(glob.glob(f"{sequence_path}/depth/*.png"))
    
#     with open(f"{sequence_path}/text.txt", 'r') as f:
#         text = f.read().strip()
    
#     # è¯»å–ç¬¬ä¸€å¸§çš„bbox
#     with open(f"{sequence_path}/groundtruth.txt", 'r') as f:
#         init_bbox = list(map(float, f.readline().strip().split(',')))
    
#     results = []
#     model.eval()
    
#     for i, (rgb_path, depth_path) in enumerate(zip(rgb_list, depth_list)):
#         rgb = preprocess_image(rgb_path)
#         depth = preprocess_depth(depth_path)
        
#         if i == 0:
#             # åˆå§‹åŒ–æ¨¡æ¿
#             template_rgb = crop_template(rgb, init_bbox)
#             template_depth = crop_template(depth, init_bbox)
        
#         # æå–æœç´¢åŒºåŸŸ
#         search_rgb = crop_search_region(rgb, prev_bbox)
#         search_depth = crop_search_region(depth, prev_bbox)
        
#         # é¢„æµ‹
#         with torch.no_grad():
#             pred_bbox, _ = model(
#                 template_rgb, template_depth, [text],
#                 search_rgb, search_depth
#             )
        
#         # åæ ‡è½¬æ¢å›åŸå›¾
#         bbox = transform_bbox(pred_bbox, prev_bbox)
#         results.append(bbox)
#         prev_bbox = bbox
    
#     return results


# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# if __name__ == '__main__':
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     os.makedirs('results', exist_ok=True)   

#     model = RGBDTextTracker().to(device)
#     model.load_state_dict(torch.load('best.pth', map_location=device))
#     seqs = [d for d in os.listdir('data/test') if os.path.isdir(os.path.join('data/test', d))]
#     for s in seqs:
#         res = test_sequence(model, f'data/test/{s}')
#         np.savetxt(f'results/{s}.txt', res, fmt='%.2f %.2f %.2f %.2f')
#     print('All results saved to results/')



# import os
# import glob
# import torch
# import numpy as np
# from models.tracker import RGBDTextTracker
# # ç¡®ä¿è¿™äº›å‡½æ•°è¢«æ­£ç¡®å¯¼å…¥
# from lib.dataset import preprocess_image, preprocess_depth, crop_template, crop_search_region, transform_bbox

# def test_sequence(model, sequence_path):
#    """æµ‹è¯•å•ä¸ªåºåˆ—ï¼ˆå·²ä¿®æ­£ï¼Œé€‚é…æ¯”èµ›è¦æ±‚ï¼‰"""
   
#    # 1. è¯»å–æ–‡ä»¶åˆ—è¡¨
#    rgb_list = sorted(glob.glob(f"{sequence_path}/color/*.jpg"))
#    depth_list = sorted(glob.glob(f"{sequence_path}/depth/*.png"))
   
#    # 2. è¯»å–æ–‡æœ¬
#    with open(f"{sequence_path}/nlp.txt", 'r') as f:
#        text = f.read().strip()

#    # 3. ã€ä¿®æ­£ã€‘è¯»å–ç¬¬ä¸€å¸§çš„ BBox
#    # æ¯”èµ›è¦æ±‚ä¼šæä¾›ç¬¬ä¸€å¸§BBoxã€‚è¿™é‡Œå‡è®¾å®ƒåœ¨ groundtruth.txt ä¸­
#    # (ä¸ä½ çš„è®­ç»ƒé›† å’Œæ—§ä»£ç  ä¿æŒä¸€è‡´)
#    # å¦‚æœæµ‹è¯•é›†çš„æ–‡ä»¶åä¸åŒï¼ˆä¾‹å¦‚ init.txtï¼‰ï¼Œè¯·ä¿®æ”¹è¿™é‡Œ
#    gt_path = f"{sequence_path}/groundtruth.txt"
#    if not os.path.exists(gt_path):
#        # å°è¯•å¤‡ç”¨åç§°
#        gt_path = os.path.join(sequence_path, "init.txt") # ä¸¾ä¾‹
#        if not os.path.exists(gt_path):
#            print(f"è­¦å‘Šï¼šåœ¨ {sequence_path} ä¸­æœªæ‰¾åˆ° groundtruth.txt æˆ– init.txtã€‚")
#            # å°è¯•ä» groundtruth_rect.txt è¯»å– (å¦‚æœæµ‹è¯•é›†å’Œè®­ç»ƒé›†æ ¼å¼ä¸€æ ·)
#            gt_path = f"{sequence_path}/groundtruth_rect.txt"
#            if not os.path.exists(gt_path):
#                 raise FileNotFoundError(f"æ‰¾ä¸åˆ° {sequence_path} çš„åˆå§‹BBoxæ–‡ä»¶")

#    with open(gt_path, 'r') as f:
#        # è¯»å–ç¬¬ä¸€è¡Œä½œä¸ºåˆå§‹BBox
#        init_bbox = list(map(float, f.readline().strip().split(',')))
   
#    model.eval()
   
#    # 4. ã€ä¿®æ­£ã€‘ç»“æœåˆ—è¡¨å¿…é¡»åŒ…å«ç¬¬ä¸€å¸§çš„BBox
#    results = [init_bbox]
   
#    # 5. ã€ä¿®æ­£ã€‘å°† prev_bbox åˆå§‹åŒ–ä¸ºç¬¬ä¸€å¸§çš„BBox
#    prev_bbox = init_bbox

#    # 6. ã€ä¿®æ­£ã€‘æ¨¡æ¿å›¾åƒï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å¾ªç¯æ—¶å¤„ç†ï¼‰
#    template_rgb = None
#    template_depth = None

#    # 7. ã€ä¿®æ­£ã€‘å¾ªç¯ä» 0 å¼€å§‹ï¼Œä½†ç¬¬ 0 å¸§åªç”¨äºåˆå§‹åŒ–æ¨¡æ¿
#    for i, (rgb_path, depth_path) in enumerate(zip(rgb_list, depth_list)):
       
#        rgb = preprocess_image(rgb_path) #
#        depth = preprocess_depth(depth_path) #

#        if i == 0:
#            # ã€ä¿®æ­£ã€‘ä½¿ç”¨ç¬¬ä¸€å¸§çš„BBoxè£å‰ªæ¨¡æ¿
#            template_rgb = crop_template(rgb, init_bbox)
#            template_depth = crop_template(depth, init_bbox)
#        else:
#            # ä»ç¬¬äºŒå¸§å¼€å§‹ï¼Œè¿›è¡Œè·Ÿè¸ª
           
#            # ã€ä¿®æ­£ã€‘ä½¿ç”¨ä¸Šä¸€å¸§çš„ prev_bbox è£å‰ªæœç´¢åŒºåŸŸ
#            search_rgb = crop_search_region(rgb, prev_bbox)
#            search_depth = crop_search_region(depth, prev_bbox)

#            with torch.no_grad():
#                pred_bbox, _ = model(
#                    template_rgb, template_depth, [text],
#                    search_rgb, search_depth
#                ) [cite: 9]

#            # [cite_start]ã€ä¿®æ­£ã€‘åæ ‡è½¬æ¢
#            bbox = transform_bbox(pred_bbox, prev_bbox)
#            prev_bbox = bbox
#            results.append(bbox) #

#    return results

# # ---------------------------------------------------
# # ä½ çš„ if __name__ == '__main__': éƒ¨åˆ†ä¸éœ€è¦å¤§æ”¹
# # ---------------------------------------------------
# if __name__ == '__main__':
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     os.makedirs('results', exist_ok=True)

#     # åŠ è½½æ¨¡å‹
#     model = RGBDTextTracker().to(device)
#     model.load_state_dict(torch.load('best.pth', map_location=device))

#     # ã€ä¿®æ­£ã€‘æŒ‡å®šåˆ°ä½ çš„ aic25 æ ¹ç›®å½•
#     test_root = '/data/depth/aic25'
    
#     # ã€ä¿®æ­£ã€‘ä¿®æ”¹æ­¤è¡Œä»¥è¿‡æ»¤æ–‡ä»¶å¤¹
#     # æˆ‘ä»¬åªé€‰æ‹©é‚£äº›æ˜¯ç›®å½•ã€ä¸”æ–‡ä»¶å¤¹åç§°æ˜¯çº¯æ•°å­—ï¼ˆä¾‹å¦‚ '001', '050'ï¼‰çš„
#     seqs = [
#         d for d in os.listdir(test_root) 
#         if os.path.isdir(os.path.join(test_root, d)) and d.isdigit()
#     ]
    
#     # ç¡®ä¿å®ƒä»¬æŒ‰é¡ºåºæ‰§è¡Œï¼ˆ'001', '002'...ï¼‰
#     seqs.sort() 

#     print(f"åœ¨ {test_root} ä¸­æ‰¾åˆ°äº† {len(seqs)} ä¸ªæµ‹è¯•åºåˆ—ã€‚")
#     if 'train' in seqs or 'val' in seqs:
#         print("è­¦å‘Šï¼š'train' æˆ– 'val' æ–‡ä»¶å¤¹è¢«é”™è¯¯åœ°åŒ…å«ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
    
#     for s in seqs:
#         seq_path = os.path.join(test_root, s)
#         print(f'Processing {s} ...')
        
#         # (ç¡®ä¿ä½ å·²ç»ä½¿ç”¨äº†æˆ‘åœ¨ä¸Šä¸€æ­¥å›å¤ä¸­æä¾›çš„ã€ä¿®æ­£åçš„ test_sequence å‡½æ•°)
#         res = test_sequence(model, seq_path)
        
#         # ç»“æœä¿å­˜
#         np.savetxt(f'results/{s}.txt', res, fmt='%.2f %.2f %.2f %.2f')

#     print('All results saved to results/')

# import os
# import glob
# import torch
# import numpy as np
# from PIL import Image
# from models.tracker_final import RGBDTextTracker
# from lib.dataset_fixed import preprocess_image, preprocess_depth

# def load_model(ckpt_path='best_final.pth', device='cuda'):
#     model = RGBDTextTracker().to(device)
#     model.load_state_dict(torch.load(ckpt_path, map_location=device))
#     model.eval()
#     return model

# def process_sequence(model, seq_dir, output_path, device='cuda'):
#     """å¤„ç†å•ä¸ªåºåˆ—"""
#     rgb_frames = sorted(glob.glob(f"{seq_dir}/color/*.jpg"))
#     depth_frames = sorted(glob.glob(f"{seq_dir}/depth/*.png"))
#     text_file = f"{seq_dir}/nlp.txt"
#     gt_file = f"{seq_dir}/groundtruth_rect.txt"
    
#     if not rgb_frames or not depth_frames:
#         print(f"[WARNING] Empty sequence: {seq_dir}")
#         return
    
#     # è¯»å–æ–‡æœ¬
#     with open(text_file, 'r') as f:
#         text = f.read().strip()
    
#     # è¯»å–åˆå§‹GT
#     with open(gt_file, 'r') as f:
#         init_bbox = list(map(float, f.readline().strip().split(',')))
    
#     # è·å–åŸå§‹å°ºå¯¸
#     orig_img = Image.open(rgb_frames[0])
#     orig_w, orig_h = orig_img.size
    
#     # ç¬¬ä¸€å¸§ä½œä¸ºæ¨¡æ¿
#     template_rgb = preprocess_image(rgb_frames[0]).unsqueeze(0).to(device)
#     template_depth = preprocess_depth(depth_frames[0]).unsqueeze(0).to(device)
    
#     results = [init_bbox]  # ç¬¬ä¸€å¸§ç”¨GT
    
#     with torch.no_grad():
#         prev_bbox_256 = [  # è½¬æ¢åˆå§‹GTåˆ°256å°ºåº¦ï¼ˆä¾›å¹³æ»‘ç”¨ï¼‰
#             init_bbox[0] * 256 / orig_w,
#             init_bbox[1] * 256 / orig_h,
#             init_bbox[2] * 256 / orig_w,
#             init_bbox[3] * 256 / orig_h
#         ]
        
#         for i in range(1, len(rgb_frames)):
#             # åŠ è½½å½“å‰å¸§
#             search_rgb = preprocess_image(rgb_frames[i]).unsqueeze(0).to(device)
#             search_depth = preprocess_depth(depth_frames[i]).unsqueeze(0).to(device)
            
#             # é¢„æµ‹ï¼ˆè¾“å‡ºæ˜¯256å°ºåº¦ï¼‰
#             pred_bbox_256, _ = model(template_rgb, template_depth, [text], search_rgb, search_depth)
#             pred_bbox_256 = pred_bbox_256.cpu().numpy()[0]
            
#             # ===== å°ºåº¦å¹³æ»‘ï¼ˆé˜²æ­¢çªå˜ï¼‰ =====
#             max_scale_change = 1.3
#             w_ratio = pred_bbox_256[2] / (prev_bbox_256[2] + 1e-6)
#             h_ratio = pred_bbox_256[3] / (prev_bbox_256[3] + 1e-6)
            
#             if w_ratio > max_scale_change:
#                 pred_bbox_256[2] = prev_bbox_256[2] * max_scale_change
#             elif w_ratio < 1/max_scale_change:
#                 pred_bbox_256[2] = prev_bbox_256[2] / max_scale_change
                
#             if h_ratio > max_scale_change:
#                 pred_bbox_256[3] = prev_bbox_256[3] * max_scale_change
#             elif h_ratio < 1/max_scale_change:
#                 pred_bbox_256[3] = prev_bbox_256[3] / max_scale_change
            
#             # ===== ä½ç½®å¹³æ»‘ =====
#             alpha = 0.7  # å¹³æ»‘ç³»æ•°
#             pred_bbox_256[0] = alpha * pred_bbox_256[0] + (1-alpha) * prev_bbox_256[0]
#             pred_bbox_256[1] = alpha * pred_bbox_256[1] + (1-alpha) * prev_bbox_256[1]
            
#             # ===== è½¬æ¢å›åŸå›¾å°ºåº¦ =====
#             scale_x = orig_w / 256.0
#             scale_y = orig_h / 256.0
#             pred_bbox_orig = [
#                 pred_bbox_256[0] * scale_x,
#                 pred_bbox_256[1] * scale_y,
#                 pred_bbox_256[2] * scale_x,
#                 pred_bbox_256[3] * scale_y
#             ]
            
#             # è¾¹ç•Œæ£€æŸ¥
#             pred_bbox_orig[0] = max(0, min(pred_bbox_orig[0], orig_w - pred_bbox_orig[2]))
#             pred_bbox_orig[1] = max(0, min(pred_bbox_orig[1], orig_h - pred_bbox_orig[3]))
#             pred_bbox_orig[2] = max(1, min(pred_bbox_orig[2], orig_w - pred_bbox_orig[0]))
#             pred_bbox_orig[3] = max(1, min(pred_bbox_orig[3], orig_h - pred_bbox_orig[1]))
            
#             results.append(pred_bbox_orig)
#             prev_bbox_256 = pred_bbox_256
            
#             # ===== è‡ªé€‚åº”æ¨¡æ¿æ›´æ–° =====
#             # æ¯10å¸§æˆ–ç½®ä¿¡åº¦é«˜æ—¶æ›´æ–°
#             if i % 10 == 0:
#                 template_rgb = search_rgb.clone()
#                 template_depth = search_depth.clone()
    
#     # ä¿å­˜ç»“æœ
#     with open(output_path, 'w') as f:
#         for bbox in results:
#             f.write(f"{bbox[0]:.2f} {bbox[1]:.2f} {bbox[2]:.2f} {bbox[3]:.2f}\n")
    
#     print(f"[OK] {os.path.basename(seq_dir)}: {len(results)} frames")

# def main():
#     device = 'cuda' if torch.cuda.is_available() else 'cpu'
#     print(f"Using device: {device}")
    
#     model = load_model('best_final.pth', device)
    
#     # æµ‹è¯•é›†è·¯å¾„
#     test_root = '/data/depth/aic25/test'
#     output_dir = 'results_final'
#     os.makedirs(output_dir, exist_ok=True)
    
#     # å¤„ç†æ‰€æœ‰æµ‹è¯•åºåˆ—
#     test_seqs = sorted(glob.glob(f"{test_root}/*"))
    
#     print(f"\nProcessing {len(test_seqs)} sequences...")
#     for seq_dir in test_seqs:
#         if not os.path.isdir(seq_dir):
#             continue
        
#         seq_name = os.path.basename(seq_dir)
#         output_path = f"{output_dir}/{seq_name}.txt"
        
#         try:
#             process_sequence(model, seq_dir, output_path, device)
#         except Exception as e:
#             print(f"[ERROR] {seq_name}: {e}")
#             # å†™å…¥é»˜è®¤å€¼é¿å…æäº¤å¤±è´¥
#             with open(output_path, 'w') as f:
#                 f.write("0.00 0.00 1.00 1.00\n")
    
#     print(f"\nâœ… All done! Results saved to {output_dir}/")
#     print(f"Now you can: cd {output_dir} && zip -r ../submission.zip *.txt")

# if __name__ == "__main__":
#     main()

# â†‘å•å¡
# import sys, os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "lib")))

# from lib.dataset import RGBDTextTestDataset

# import torch
# import time
# from tqdm import tqdm
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from dataset import RGBDTextTestDataset

# def test_fast(
#     data_root="/data/depth/aic25",
#     output_dir="results/",
#     batch_size=4,
#     num_workers=8,
#     use_amp=True,
# ):
#     """é«˜æ•ˆåŒå¡æµ‹è¯•"""
#     torch.backends.cudnn.benchmark = True
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     print("ğŸš€ Initializing RGBDTextTracker for fast testing...")

#     # 1. åŠ è½½æ¨¡å‹
#     model = RGBDTextTracker()
#     ckpt = torch.load("/root/ost/RGBDTextTracker/best.pth", map_location=device)
#     state_dict = ckpt["model"] if "model" in ckpt else ckpt
#     model.load_state_dict(state_dict, strict=False)
#     model.eval()

#     # 2. å¯ç”¨åŒå¡å¹¶è¡Œ
#     if torch.cuda.device_count() > 1:
#         print(f"ğŸ”§ Using {torch.cuda.device_count()} GPUs for inference")
#         model = torch.nn.DataParallel(model)
#     model = model.to(device)

#     # 3. æ•°æ®åŠ è½½
#     dataset = RGBDTextTestDataset(data_root)
#     dataloader = DataLoader(
#         dataset,
#         batch_size=batch_size,
#         shuffle=False,
#         num_workers=num_workers,
#         pin_memory=True,
#         drop_last=False,
#     )

#     os.makedirs(output_dir, exist_ok=True)

#     print("âœ… Model & Data Ready, Start Inference ...")
#     start_time = time.time()

#     with torch.no_grad():
#         for batch in tqdm(dataloader, ncols=100):
#             tpl_rgb = batch["template_rgb"].to(device, non_blocking=True)
#             tpl_dep = batch["template_depth"].to(device, non_blocking=True)
#             srh_rgb = batch["search_rgb"].to(device, non_blocking=True)
#             srh_dep = batch["search_depth"].to(device, non_blocking=True)
#             text = batch["text"]

#             # AMP åŠ é€Ÿæ¨ç†
#             if use_amp:
#                 with torch.cuda.amp.autocast():
#                     pred_bbox, _ = model(tpl_rgb, tpl_dep, text, srh_rgb, srh_dep)
#             else:
#                 pred_bbox, _ = model(tpl_rgb, tpl_dep, text, srh_rgb, srh_dep)

#             # ä¿å­˜é¢„æµ‹
#             for i, bbox in enumerate(pred_bbox):
#                 vid, frame_id = batch["video_id"][i], batch["frame_id"][i]
#                 save_path = os.path.join(output_dir, f"{vid}.txt")
#                 with open(save_path, "a") as f:
#                     x, y, w, h = bbox.tolist()
#                     f.write(f"{frame_id},{x:.2f},{y:.2f},{w:.2f},{h:.2f}\n")

#     total_time = time.time() - start_time
#     print(f"\nğŸ Done! Total time: {total_time/60:.1f} min "
#           f"({len(dataset)/(total_time):.2f} FPS total)\n")

# if __name__ == "__main__":
#     test_fast(
#         data_root="/data/depth/aic25",  # ä½ çš„æµ‹è¯•é›†è·¯å¾„
#         output_dir="results_final/",
#         batch_size=16,                  # å…³é”®ï¼å¤šå¡åŠ å¤§batch
#         num_workers=16,
#         use_amp=True
#     )


# import sys, os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), ".")))

# import torch
# import time
# from tqdm import tqdm
# from torch.utils.data import DataLoader
# from models.tracker import RGBDTextTracker
# from lib.dataset import RGBDTextTestDataset
# import numpy as np
# import glob

# def test_fast(
#     data_root="/data/depth/test",
#     output_dir="results/",
#     batch_size=8,
#     num_workers=8,
#     use_amp=True,
# ):
#     """é«˜æ•ˆæµ‹è¯•"""
#     torch.backends.cudnn.benchmark = True
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#     print("ğŸš€ Initializing RGBDTextTracker for fast testing...")

#     # 1. åŠ è½½æ¨¡å‹
#     model = RGBDTextTracker()
#     ckpt = torch.load("/root/ost/RGBDTextTracker/best.pth", map_location=device)
    
#     # âœ… å¤„ç†å¯èƒ½çš„æƒé‡æ ¼å¼
#     if isinstance(ckpt, dict) and "model" in ckpt:
#         state_dict = ckpt["model"]
#     elif isinstance(ckpt, dict) and "state_dict" in ckpt:
#         state_dict = ckpt["state_dict"]
#     else:
#         state_dict = ckpt
    
#     # âœ… ç§»é™¤ DataParallel åŒ…è£…ï¼ˆå¦‚æœæœ‰ï¼‰
#     from collections import OrderedDict
#     new_state_dict = OrderedDict()
#     for k, v in state_dict.items():
#         name = k.replace("module.", "") if k.startswith("module.") else k
#         new_state_dict[name] = v
    
#     model.load_state_dict(new_state_dict, strict=False)
#     model.eval()

#     # 2. å¯ç”¨åŒå¡å¹¶è¡Œ
#     if torch.cuda.device_count() > 1:
#         print(f"ğŸ”§ Using {torch.cuda.device_count()} GPUs for inference")
#         model = torch.nn.DataParallel(model)
#     model = model.to(device)

#     # 3. æŒ‰åºåˆ—å¤„ç†ï¼ˆè€Œébatchï¼‰
#     os.makedirs(output_dir, exist_ok=True)
    
#     # âœ… è·å–æ‰€æœ‰åºåˆ—
#     seq_dirs = sorted([
#         os.path.join(data_root, d) 
#         for d in os.listdir(data_root) 
#         if os.path.isdir(os.path.join(data_root, d))
#     ])
    
#     print(f"âœ… Found {len(seq_dirs)} sequences")
#     print("âœ… Model & Data Ready, Start Inference ...")
#     start_time = time.time()

#     with torch.no_grad():
#         for seq_path in tqdm(seq_dirs, desc="Processing", ncols=100):
#             seq_name = os.path.basename(seq_path)
            
#             # è¯»å–åºåˆ—ä¿¡æ¯
#             # âœ… è‡ªåŠ¨é€’å½’æ‰¾ color/depth æ–‡ä»¶å¤¹ï¼Œæ— è®ºå±‚æ¬¡
#             rgb_frames = sorted(glob.glob(os.path.join(seq_path, "**/color/*.*"), recursive=True))
#             depth_frames = sorted(glob.glob(os.path.join(seq_path, "**/depth/*.*"), recursive=True))

#             # âœ… è°ƒè¯•è¾“å‡ºï¼šç¡®è®¤æ¯ä¸ªåºåˆ—æ‰¾åˆ°å¤šå°‘å¸§
#             print(f"ğŸ¯ {seq_name}: {len(rgb_frames)} RGB, {len(depth_frames)} Depth")

#             if len(rgb_frames) == 0 or len(depth_frames) == 0:
#                 print(f"[WARN] Empty sequence: {seq_name}")
#                 continue

#             # è¯»å–æ–‡æœ¬
#             text_file = f"{seq_path}/nlp.txt"
#             with open(text_file, 'r') as f:
#                 text = f.read().strip()
            
#             # è¯»å–åˆå§‹bbox
#             gt_file = f"{seq_path}/groundtruth.txt"
#             with open(gt_file, 'r') as f:
#                 init_bbox = list(map(float, f.readline().strip().split(',')))
            
#             # è·å–åŸå›¾å°ºå¯¸
#             from PIL import Image
#             orig_img = Image.open(rgb_frames[0])
#             orig_w, orig_h = orig_img.size
            
#             # é¢„å¤„ç†ç¬¬ä¸€å¸§ä½œä¸ºæ¨¡æ¿
#             from lib.dataset import preprocess_image, preprocess_depth
#             template_rgb = preprocess_image(rgb_frames[0]).unsqueeze(0).to(device)
#             template_depth = preprocess_depth(depth_frames[0]).unsqueeze(0).to(device)
            
#             results = [init_bbox]  # ç¬¬ä¸€å¸§ç”¨GT
            
#             # é€å¸§é¢„æµ‹
#             for i in range(1, len(rgb_frames)):
#                 search_rgb = preprocess_image(rgb_frames[i]).unsqueeze(0).to(device)
#                 search_depth = preprocess_depth(depth_frames[i]).unsqueeze(0).to(device)
                
#                 # AMP åŠ é€Ÿ
#                 if use_amp:
#                     with torch.cuda.amp.autocast():
#                         pred_bbox_norm, _ = model(template_rgb, template_depth, [text], search_rgb, search_depth)
#                 else:
#                     pred_bbox_norm, _ = model(template_rgb, template_depth, [text], search_rgb, search_depth)
                
#                 # âœ… è½¬æ¢åˆ°åŸå›¾å°ºåº¦
#                 pred_bbox_norm = pred_bbox_norm.cpu().numpy()[0]  # [0-1] å½’ä¸€åŒ–
#                 pred_bbox = [
#                     pred_bbox_norm[0] * orig_w,
#                     pred_bbox_norm[1] * orig_h,
#                     pred_bbox_norm[2] * orig_w,
#                     pred_bbox_norm[3] * orig_h
#                 ]
                
#                 # è¾¹ç•Œæ£€æŸ¥
#                 pred_bbox[0] = max(0, min(pred_bbox[0], orig_w - pred_bbox[2]))
#                 pred_bbox[1] = max(0, min(pred_bbox[1], orig_h - pred_bbox[3]))
#                 pred_bbox[2] = max(1, min(pred_bbox[2], orig_w - pred_bbox[0]))
#                 pred_bbox[3] = max(1, min(pred_bbox[3], orig_h - pred_bbox[1]))
                
#                 results.append(pred_bbox)
                
#                 # æ¯10å¸§æ›´æ–°æ¨¡æ¿
#                 if i % 10 == 0:
#                     template_rgb = search_rgb.clone()
#                     template_depth = search_depth.clone()
            
#             # ä¿å­˜ç»“æœ
#             output_file = f"{output_dir}/{seq_name}.txt"
#             with open(output_file, 'w') as f:
#                 for bbox in results:
#                     f.write(f"{bbox[0]:.2f} {bbox[1]:.2f} {bbox[2]:.2f} {bbox[3]:.2f}\n")

#     total_time = time.time() - start_time
#     print(f"\nğŸ Done! Total time: {total_time/60:.1f} min\n")

# if __name__ == "__main__":
#     test_fast(
#         data_root="/data/depth/test",
#         output_dir="results_final/",
#         batch_size=8,
#         num_workers=8,
#         use_amp=True
#     )

import sys, os, glob, time
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), ".")))
import re
import torch
from PIL import Image
import numpy as np
from tqdm import tqdm
from models.tracker import RGBDTextTracker
from lib.dataset import preprocess_image, preprocess_depth

def test_fast_single(
    data_root="/data/depth/test",
    output_dir="results_final/",
    model_path="/root/ost/RGBDTextTracker/best.pth",
):
    """å•å¡é«˜æ•ˆæ¨ç†ç‰ˆï¼ˆç¨³å®šå¿«é€Ÿï¼Œ15~20åˆ†é’Ÿå†…å®Œæˆï¼‰"""
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.enabled = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    print("ğŸš€ Initializing model...")
    model = RGBDTextTracker().to(device)
    ckpt = torch.load(model_path, map_location=device)

    # æ”¯æŒå¤šç§æƒé‡æ ¼å¼
    if isinstance(ckpt, dict):
        if "model" in ckpt:
            ckpt = ckpt["model"]
        elif "state_dict" in ckpt:
            ckpt = ckpt["state_dict"]

    # å»é™¤ "module." å‰ç¼€
    from collections import OrderedDict
    new_state_dict = OrderedDict()
    for k, v in ckpt.items():
        name = k.replace("module.", "") if k.startswith("module.") else k
        new_state_dict[name] = v
    model.load_state_dict(new_state_dict, strict=False)
    model.eval().half()

    os.makedirs(output_dir, exist_ok=True)
    print(f"âš™ï¸ Using FP16 + cudnn.benchmark, writing results to {output_dir}")

    seq_dirs = sorted([
        os.path.join(data_root, d)
        for d in os.listdir(data_root)
        if os.path.isdir(os.path.join(data_root, d))
    ])
    print(f"âœ… Found {len(seq_dirs)} sequences")
    start_time = time.time()

    with torch.no_grad():
        for seq_path in tqdm(seq_dirs, desc="Processing", ncols=100):
            seq_name = os.path.basename(seq_path)
            def get_all_images(folder):
                files = []
                for ext in ["jpg", "jpeg", "png", "bmp", "JPG", "PNG", "JPEG"]:
                    files.extend(glob.glob(os.path.join(folder, f"**/*.{ext}"), recursive=True))
                files = sorted(files, key=lambda x: int(re.sub(r'\D', '', os.path.basename(x)) or 0))
                return files

            rgb_frames = get_all_images(os.path.join(seq_path, "color"))
            depth_frames = get_all_images(os.path.join(seq_path, "depth"))


            text_file = f"{seq_path}/nlp.txt"
            gt_file = f"{seq_path}/groundtruth.txt"

            if not (os.path.exists(text_file) and os.path.exists(gt_file) and rgb_frames and depth_frames):
                print(f"[WARN] Skipped {seq_name}, missing files")
                continue

            with open(text_file, "r") as f:
                text = f.read().strip()
            with open(gt_file, "r") as f:
                init_bbox = list(map(float, f.readline().strip().split(",")))

            orig_img = Image.open(rgb_frames[0])
            orig_w, orig_h = orig_img.size

            # æ¨¡æ¿å¸§
            tpl_rgb = preprocess_image(rgb_frames[0]).unsqueeze(0).to(device, non_blocking=True).half()
            tpl_dep = preprocess_depth(depth_frames[0]).unsqueeze(0).to(device, non_blocking=True).half()

            results = [init_bbox]

            for i in range(1, len(rgb_frames)):
                srgb = preprocess_image(rgb_frames[i]).unsqueeze(0).to(device, non_blocking=True).half()
                sdep = preprocess_depth(depth_frames[i]).unsqueeze(0).to(device, non_blocking=True).half()

                with torch.cuda.amp.autocast():
                    pred_bbox_norm, _ = model(tpl_rgb, tpl_dep, [text], srgb, sdep)

                pred_bbox_norm = pred_bbox_norm.cpu().numpy()[0]
                pred_bbox = [
                    pred_bbox_norm[0] * orig_w,
                    pred_bbox_norm[1] * orig_h,
                    pred_bbox_norm[2] * orig_w,
                    pred_bbox_norm[3] * orig_h,
                ]

                # è¾¹ç•Œæ£€æŸ¥
                pred_bbox[0] = max(0, min(pred_bbox[0], orig_w - pred_bbox[2]))
                pred_bbox[1] = max(0, min(pred_bbox[1], orig_h - pred_bbox[3]))
                pred_bbox[2] = max(1, min(pred_bbox[2], orig_w - pred_bbox[0]))
                pred_bbox[3] = max(1, min(pred_bbox[3], orig_h - pred_bbox[1]))

                results.append(pred_bbox)

                # æ¯10å¸§æ›´æ–°æ¨¡æ¿
                if i % 10 == 0:
                    tpl_rgb = srgb.clone()
                    tpl_dep = sdep.clone()

            # ä¿å­˜ç»“æœ
            out_path = os.path.join(output_dir, f"{seq_name}.txt")
            with open(out_path, "w") as f:
                for b in results:
                    f.write(f"{b[0]:.2f} {b[1]:.2f} {b[2]:.2f} {b[3]:.2f}\n")

    print(f"\nğŸ Done! Total time: {(time.time()-start_time)/60:.1f} min")


if __name__ == "__main__":
    test_fast_single(
        data_root="/data/depth/test",
        output_dir="results_final/",
        model_path="/root/ost/RGBDTextTracker/best.pth",
    )
